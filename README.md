# Exploration and Comparison of Transformers for Image Classification
This repostiry contains the complete source code for the  master thesis ***Exploration and Comparison of Transformers for Image Classification*** by David PosluÅ¡nÃ½ at [***Faculty of Informatics and Statistics of Prague University of Economics and Business***](https://fis.vse.cz).


This thesis focuses on the research and application of Transformers in image classification. The main goal is to evaluate the performance of selected Transformer models on diverse image classification datasets in a series of hand-crafted transfer learning experiments. The results of the analysis are compared and further discussed in the context of available literature. The first part of the thesis presents a theoretical background of Transformers. It contains an overview of the original architecture and its adaptation to computer vision (CV). An extensive literature survey of Transformers in CV is performed, focusing primarily on image classification. The survey provides a basis for selecting models used in the experiments. The second part reviews available image classification datasets to select diverse domain-specific challenges for the models. Subsequently, experiments based on transfer learning are designed to evaluate and compare the models in various experimental setups. The thesis concludes by applying the selected models to the unseen data. The results are evaluated, analyzed, and discussed based on the context of the individual experiments. The primary contribution of the thesis is an in-depth evaluation of key Transformer models adopted for image classification on diverse domain-specific datasets. The complex experiments analyze the models under different conditions and compare their generalization performance outside of the training distribution. The outcome serves as a summary of the most important Transformer-based architectures for image classification.

## Table of Contents
  * [Models](#models)
  * [Datasets](#datasets)
  * [Implementation](#implementation)
  * [Experiments](#experiments)
    + [Linear probing](#linear-probing)
    + [Fine-tuning](#fine-tuning)
    + [Few-shot linear probing](#few-shot-linear-probing)
    + [Fine-tuning with data augmentations](#fine-tuning-with-data-augmentations)
    + [Zero-shot transfer](#zero-shot-transfer)
  * [Configurations](#configurations)
  * [Prerequisites](#prerequisites)
  * [Results](#results)
  * [Requirements](#requirements)
  * [How to Run the Project](#how-to-run-the-project)
  * [License](#license)
  * [Acknowledgements](#acknowledgements)
  * [Authors](#authors)


## Models
- **Vision Transformer (ViT)**
    - https://huggingface.co/google/vit-base-patch16-224
- **Data-efficient image Transformer (DeiT)**
    - https://huggingface.co/facebook/deit-base-distilled-patch16-224
- **Swin Transformer (Swin)**
    - https://huggingface.co/microsoft/swin-base-patch4-window7-224
- **Contrastive Language-Image Pre-training (CLIP)**
    - https://huggingface.co/openai/clip-vit-base-patch16

## Datasets
- **RESISC45**
    - https://huggingface.co/datasets/timm/resisc45
- **Food-101**
    - https://huggingface.co/datasets/ethz/food101
- **FER2013**
    - https://huggingface.co/datasets/AutumnQiu/fer2013
- **PatchCamelyon**
    - https://huggingface.co/datasets/zacharielegault/PatchCamelyon 
- **SUN397**
    - https://huggingface.co/datasets/dpdl-benchmark/sun397 
- **DTD**
    - https://huggingface.co/datasets/tanganke/dtd
 

## Implementation
The main logic behind the implementation in the exeperiments is located in:

```
â”œâ”€â”€ ðŸ“‚ src
â”‚   â”œâ”€â”€ ðŸ“„ __init__.py
â”‚   â”œâ”€â”€ ðŸ“„ dataset_builder.py
â”‚   â”œâ”€â”€ ðŸ“„ models.py
â”‚   â””â”€â”€ ðŸ“„ train.py
â””â”€â”€ ðŸ“‚ utils
    â”œâ”€â”€ ðŸ“„ __init__.py
    â”œâ”€â”€ ðŸ“„ config.py
    â”œâ”€â”€ ðŸ“„ data_utils.py
    â”œâ”€â”€ ðŸ“„ models_utils.py
    â”œâ”€â”€ ðŸ“„ train_utils.py
    â””â”€â”€ ðŸ“„ visualization_utils.py
```
 
## Experiments

### Linear probing
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ benchmarks
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ linear-probing
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ linear_probing_clip.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ linear_probing_deit.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ linear_probing_results.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ linear_probing_swin.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ linear_probing_vit.ipynb
```
### Fine-tuning
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ benchmarks
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ fine-tuning
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_clip.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_deit.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_results.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_swin.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ fine_tuning_vit.ipynb
```
### Few-shot linear probing
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ benchmarks
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ few-shot-linear-probing
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ few_shot_linear_probing_clip.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ few_shot_linear_probing_deit.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ few_shot_linear_probing_results.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ few_shot_linear_probing_swin.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ few_shot_linear_probing_vit.ipynb
```
### Fine-tuning with data augmentations
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ benchmarks
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ fine-tuning-data-aug
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_data_aug_clip.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_data_aug_deit.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_data_aug_results.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_data_aug_swin.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ fine_tuning_data_aug_vit.ipynb
```
### Zero-shot transfer
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ benchmarks
â”‚   â”‚   â””â”€â”€ ðŸ“‚ zero-shot-transfer
â”‚   â”‚       â””â”€â”€ ðŸ“„ zero_shot_transfer.ipynb
```

## Configurations
All experiment and model settings are stored in:
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“‚ configs
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ data_augmentations.json
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ few_shot_linear_probing.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fine_tuning_data_aug.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ linear_probing.json
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ zero_shot_transfer.json
â”‚   â”‚   â””â”€â”€ ðŸ“‚ processors
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ clip.json
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ deit.json
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ swin.json
â”‚   â”‚       â””â”€â”€ ðŸ“„ vit.json
```

## Prerequisites
`install_packages.ipynb` 
- install all necessary packages

`load_datasets.ipynb` 
- pre-load all the available datasets

`load_models.ipynb` 
- pre-load all the available models and their processors

```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â””â”€â”€ ðŸ“‚ prerequisites
â”‚       â”œâ”€â”€ ðŸ“„ install_packages.ipynb 
â”‚       â”œâ”€â”€ ðŸ“„ load_datasets.ipynb
â”‚       â””â”€â”€ ðŸ“„ load_models.ipynb
```

## Results
The complete results with additional analyses are avaialble in Jupyter nootebok file located at:
```
â”œâ”€â”€ ðŸ“‚ experiments
â”‚   â”œâ”€â”€ ðŸ“„ experiments_results.ipynb
```

For the results of individual experiments, please refer to specific Jupyter notebooks in `experiments/benchmarks/` folder.

## Requirements
To run this project, ensure the following requirements are installed:
- Python 3.10 or higher
- Dependencies listed in `requirements.txt`

Install the dependencies using this command:
```
pip install -r requirements.txt
```

## How to Run the Project
1. Clone the repository:
    ```
    git clone https://github.com/itsDaiton/masters-thesis.git
    cd masters-thesis
    ```
2. Install the prerequisites for the experiments:
    ```
    cd experiments/prerequisites
    jupyter notebook install_packages.ipynb 
    jupyter notebook load_datasets.ipynb
    jupyter notebook load_models.ipynb
    ```
3.  Run an experiment, e.g.:
    ```
    cd experiments/benchmarks/fine-tuning
    jupyter notebook fine_tuning_vit.ipynb
    ```

## License
The project is distributed under the MIT License. See `LICENSE` for more information.

## Acknowledgements
Thesis supervisor:  doc. Ing. OndÅ™ej Zamazal, Ph.D.

This work was also supported by the Ministry of Education, Youth and Sports of the Czech
Republic through the e-INFRA CZ (ID:90254).

## Authors
Bc. David PosluÅ¡nÃ½ <br>
Study program: Knowledge and Web Technologies <br>
Specialization: Quantitative Analysis <br>
[Prague University of Economics and Business Faculty of Informatics and Statistics](https://fis.vse.cz/)
